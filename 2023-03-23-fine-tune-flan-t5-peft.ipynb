{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LoRA å’Œ Hugging Face é«˜æ•ˆè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨[å¤§è¯­è¨€æ¨¡å‹ä½ç§©é€‚é…ï¼ˆLow-Rank Adaptation of Large Language Modelsï¼ŒLoRAï¼‰](https://arxiv.org/abs/2106.09685)æŠ€æœ¯åœ¨å• GPU ä¸Šå¾®è°ƒ 110 äº¿å‚æ•°çš„ FLAN-T5 XXL æ¨¡å‹ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨åˆ° Hugging Face çš„ [Transformers](https://huggingface.co/docs/transformers/index)ã€[Accelerate](https://huggingface.co/docs/accelerate/index) å’Œ [PEFT](https://github.com/huggingface/peft) åº“ã€‚\n",
    "\n",
    "é€šè¿‡æœ¬æ–‡ï¼Œä½ ä¼šå­¦åˆ°ï¼š\n",
    "\n",
    "1. å¦‚ä½•æ­å»ºå¼€å‘ç¯å¢ƒ\n",
    "2. å¦‚ä½•åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†\n",
    "3. å¦‚ä½•ä½¿ç”¨ LoRA å’Œ bnbï¼ˆå³bitsandbytesï¼‰ int-8 å¾®è°ƒ T5\n",
    "4. å¦‚ä½•è¯„ä¼° LoRA FLAN-T5 å¹¶å°†å…¶ç”¨äºæ¨ç†\n",
    "5. å¦‚ä½•æ¯”è¾ƒä¸åŒæ–¹æ¡ˆçš„æ€§ä»·æ¯”\n",
    "\n",
    "### å¿«é€Ÿå…¥é—¨ï¼šè½»é‡åŒ–å¾®è°ƒï¼ˆParameter Efficient Fine-Tuningï¼ŒPEFTï¼‰\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft) æ˜¯ Hugging Face çš„ä¸€ä¸ªæ–°çš„å¼€æºåº“ã€‚ä½¿ç”¨ PEFT åº“ï¼Œæ— éœ€å¾®è°ƒæ¨¡å‹çš„å…¨éƒ¨å‚æ•°ï¼Œå³å¯é«˜æ•ˆåœ°å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (Pre-trained Language Modelï¼ŒPLM) é€‚é…åˆ°å„ç§ä¸‹æ¸¸åº”ç”¨ã€‚PEFT ç›®å‰æ”¯æŒä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š\n",
    "\n",
    "- LoRAï¼š[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuningï¼š[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuningï¼š[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuningï¼š[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "\n",
    "*æ³¨æ„ï¼šæœ¬æ•™ç¨‹æ˜¯åœ¨ g5.2xlarge AWS EC2 å®ä¾‹ä¸Šåˆ›å»ºå’Œè¿è¡Œçš„ï¼Œè¯¥å®ä¾‹åŒ…å« 1 ä¸ª NVIDIA A10Gã€‚*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ­å»ºå¼€å‘ç¯å¢ƒ\n",
    "\n",
    "åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ AWS é¢„ç½®çš„ [PyTorch æ·±åº¦å­¦ä¹  AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html)ï¼Œå…¶å·²å®‰è£…äº†æ­£ç¡®çš„ CUDA é©±åŠ¨ç¨‹åºå’Œ PyTorchã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿˜éœ€è¦å®‰è£…ä¸€äº› Hugging Face åº“ï¼ŒåŒ…æ‹¬ transformers å’Œ datasetsã€‚è¿è¡Œä¸‹é¢çš„ä»£ç å°±å¯å®‰è£…æ‰€æœ‰éœ€è¦çš„åŒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to c:\\users\\11985\\appdata\\local\\temp\\pip-req-build-3bwf80qe\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 34027fe813756897767b9a6f19ae7f1c4c7b418c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (23.0)\n",
      "Requirement already satisfied: psutil in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (1.13.1+cu116)\n",
      "Requirement already satisfied: transformers in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (4.26.1)\n",
      "Requirement already satisfied: accelerate in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (0.17.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\program files\\python3.7.9\\lib\\site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: filelock in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: requests in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (6.0.0)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python3.7.9\\lib\\site-packages (from tqdm>=4.27->transformers->peft==0.3.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\program files\\python3.7.9\\lib\\site-packages (from importlib-metadata->transformers->peft==0.3.0.dev0) (3.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests->transformers->peft==0.3.0.dev0) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests->transformers->peft==0.3.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests->transformers->peft==0.3.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests->transformers->peft==0.3.0.dev0) (2022.12.7)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml): started\n",
      "  Building wheel for peft (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=50810 sha256=b4a898041e90c0419af2e2eb4da857ded2f8390f40416a90cc7311624c9c766c\n",
      "  Stored in directory: C:\\Users\\11985\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-bgzptlev\\wheels\\13\\73\\6d\\ff27a3703d8bad21d7e0c24cbd9dde5d7ae78f756405707a0c\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.3.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git 'C:\\Users\\11985\\AppData\\Local\\Temp\\pip-req-build-3bwf80qe'\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in d:\\program files\\python3.7.9\\lib\\site-packages (0.1.2)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 142.7 kB/s eta 0:00:00\n",
      "Collecting py7zr\n",
      "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
      "     -------------------------------------- 66.4/66.4 kB 905.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py in d:\\program files\\python3.7.9\\lib\\site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in d:\\program files\\python3.7.9\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in d:\\program files\\python3.7.9\\lib\\site-packages (from rouge-score) (1.21.6)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Collecting grpcio>=1.24.3 (from tensorboard)\n",
      "  Downloading grpcio-1.54.0-cp37-cp37m-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 2.4 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard)\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "     -------------------------------------- 178.2/178.2 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.9/93.9 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in d:\\program files\\python3.7.9\\lib\\site-packages (from tensorboard) (3.19.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from tensorboard) (2.28.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from tensorboard) (47.1.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "     -------------------------------------- 233.6/233.6 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting wheel>=0.26 (from tensorboard)\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting texttable (from py7zr)\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pycryptodomex>=3.6.6 (from py7zr)\n",
      "  Downloading pycryptodomex-3.17-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting pyzstd>=0.14.4 (from py7zr)\n",
      "  Downloading pyzstd-0.15.6-cp37-cp37m-win_amd64.whl (231 kB)\n",
      "     -------------------------------------- 231.8/231.8 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n",
      "  Downloading pyppmd-1.0.0-cp37-cp37m-win_amd64.whl (46 kB)\n",
      "     ---------------------------------------- 46.1/46.1 kB ? eta 0:00:00\n",
      "Collecting pybcj>=0.6.0 (from py7zr)\n",
      "  Downloading pybcj-1.0.1-cp37-cp37m-win_amd64.whl (24 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.0.9 (from py7zr)\n",
      "  Downloading Brotli-1.0.9-cp37-cp37m-win_amd64.whl (365 kB)\n",
      "     -------------------------------------- 365.3/365.3 kB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata in d:\\program files\\python3.7.9\\lib\\site-packages (from py7zr) (6.0.0)\n",
      "Collecting inflate64>=0.3.1 (from py7zr)\n",
      "  Downloading inflate64-0.3.1-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: psutil in d:\\program files\\python3.7.9\\lib\\site-packages (from py7zr) (5.9.4)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\program files\\python3.7.9\\lib\\site-packages (from importlib-metadata->py7zr) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in d:\\program files\\python3.7.9\\lib\\site-packages (from importlib-metadata->py7zr) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\program files\\python3.7.9\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: click in d:\\program files\\python3.7.9\\lib\\site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: joblib in d:\\program files\\python3.7.9\\lib\\site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\program files\\python3.7.9\\lib\\site-packages (from nltk->rouge-score) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in d:\\program files\\python3.7.9\\lib\\site-packages (from nltk->rouge-score) (4.65.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in d:\\program files\\python3.7.9\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Installing collected packages: texttable, tensorboard-plugin-wit, pyasn1, brotli, wheel, werkzeug, tensorboard-data-server, rsa, pyzstd, pyppmd, pycryptodomex, pyasn1-modules, oauthlib, multivolumefile, grpcio, cachetools, requests-oauthlib, pybcj, markdown, inflate64, google-auth, py7zr, google-auth-oauthlib, tensorboard\n",
      "Successfully installed brotli-1.0.9 cachetools-5.3.0 google-auth-2.17.3 google-auth-oauthlib-0.4.6 grpcio-1.54.0 inflate64-0.3.1 markdown-3.4.3 multivolumefile-0.2.3 oauthlib-3.2.2 py7zr-0.20.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 pybcj-1.0.1 pycryptodomex-3.17 pyppmd-1.0.0 pyzstd-0.15.6 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 texttable-1.6.7 werkzeug-2.2.3 wheel-0.40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# install Hugging Face Libraries\n",
    "!pip install  git+https://github.com/huggingface/peft.git\n",
    "!pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "# install additional dependencies needed for training\n",
    "!pip install rouge-score tensorboard py7zr "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ [samsum](https://huggingface.co/datasets/samsum) æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§çº¦ 16k ä¸ªå«æ‘˜è¦çš„èŠå¤©ç±»å¯¹è¯æ•°æ®ã€‚è¿™äº›å¯¹è¯ç”±ç²¾é€šè‹±è¯­çš„è¯­è¨€å­¦å®¶åˆ¶ä½œã€‚\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Datasets åº“ä¸­çš„ *â€‹`load_dataset()`* æ–¹æ³•æ¥åŠ è½½ `samsum` æ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (D:/ProgramData/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f062668910240f4a157fdc9eb8e7608"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Train dataset size: 14732\n",
    "# Test dataset size: 819"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬è¦ç”¨ ğŸ¤— Transformers Tokenizer å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºè¯å…ƒ IDã€‚å¦‚æœä½ éœ€è¦äº†è§£è¿™ä¸€æ–¹é¢çš„çŸ¥è¯†ï¼Œè¯·ç§»æ­¥ Hugging Face è¯¾ç¨‹çš„ **[ç¬¬ 6 ç« ](https://huggingface.co/course/chapter6/1?fw=tf)**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-xxl\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-XL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ç”Ÿæˆå¼æ–‡æœ¬æ‘˜è¦å±äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬å°†æ–‡æœ¬è¾“å…¥ç»™æ¨¡å‹ï¼Œæ¨¡å‹ä¼šè¾“å‡ºæ‘˜è¦ã€‚æˆ‘ä»¬éœ€è¦äº†è§£è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬çš„é•¿åº¦ä¿¡æ¯ï¼Œä»¥åˆ©äºæˆ‘ä»¬é«˜æ•ˆåœ°æ‰¹é‡å¤„ç†è¿™äº›æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\ProgramData\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\\cache-03b5cc31bf151e08.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 255\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a41ec54c3fb4bbbb3c3c913fc294285"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 50\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å°†åœ¨è®­ç»ƒå‰ç»Ÿä¸€å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†å¹¶å°†é¢„å¤„ç†åçš„æ•°æ®é›†ä¿å­˜åˆ°ç£ç›˜ã€‚ä½ å¯ä»¥åœ¨æœ¬åœ°æœºå™¨æˆ– CPU ä¸Šè¿è¡Œæ­¤æ­¥éª¤å¹¶å°†å…¶ä¸Šä¼ åˆ° [Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\ProgramData\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\\cache-df829843b4810807.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26495fabc22646a4aaa89a1e2f3e1aca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\ProgramData\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\\cache-c6d129a5cd66b800.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/14732 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7eaa85b31dd4c00a13cc8bd62b4703a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99d952f458f840ce92a56651eb2c6c38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä½¿ç”¨ LoRA å’Œ bnb int-8 å¾®è°ƒ T5\n",
    "\n",
    "é™¤äº† LoRA æŠ€æœ¯ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) æŠŠå†»ç»“çš„ LLM é‡åŒ–ä¸º int8ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°† FLAN-T5 XXL æ‰€éœ€çš„å†…å­˜é™ä½åˆ°çº¦å››åˆ†ä¹‹ä¸€ã€‚\n",
    "\n",
    "è®­ç»ƒçš„ç¬¬ä¸€æ­¥æ˜¯åŠ è½½æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ [philschmid/flan-t5-xxl-sharded-fp16](https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16) æ¨¡å‹ï¼Œå®ƒæ˜¯ [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) çš„åˆ†ç‰‡ç‰ˆã€‚åˆ†ç‰‡å¯ä»¥è®©æˆ‘ä»¬åœ¨åŠ è½½æ¨¡å‹æ—¶ä¸è€—å°½å†…å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "299e297a01cf4e888e7570a145caf076"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `peft` ä¸º LoRA int-8 è®­ç»ƒä½œå‡†å¤‡äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r=16, \n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚ä½ æ‰€è§ï¼Œè¿™é‡Œæˆ‘ä»¬åªè®­ç»ƒäº†æ¨¡å‹å‚æ•°çš„ 0.16%ï¼è¿™ä¸ªå·¨å¤§çš„å†…å­˜å¢ç›Šè®©æˆ‘ä»¬å®‰å¿ƒåœ°å¾®è°ƒæ¨¡å‹ï¼Œè€Œä¸ç”¨æ‹…å¿ƒå†…å­˜é—®é¢˜ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥éœ€è¦åˆ›å»ºä¸€ä¸ª `DataCollatâ€‹â€‹or`ï¼Œè´Ÿè´£å¯¹è¾“å…¥å’Œæ ‡ç­¾è¿›è¡Œå¡«å……ï¼Œæˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Transformers åº“ä¸­çš„`DataCollatâ€‹â€‹orForSeq2Seq` æ¥å®Œæˆè¿™ä¸€ç¯èŠ‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åä¸€æ­¥æ˜¯å®šä¹‰è®­ç»ƒè¶…å‚ (`TrainingArguments`)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"lora-flan-t5-xxl\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\t\tauto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œå¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå¯¹äº T5ï¼Œå‡ºäºæ”¶æ•›ç¨³å®šæ€§è€ƒé‡ï¼ŒæŸäº›å±‚æˆ‘ä»¬ä»ä¿æŒ `float32` ç²¾åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python3.7.9\\lib\\site-packages\\transformers\\optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='9210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/9210 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=9210, training_loss=0.99450243888797, metrics={'train_runtime': 22422.9438, 'train_samples_per_second': 3.285, 'train_steps_per_second': 0.411, 'total_flos': 1.2471170806421914e+18, 'train_loss': 0.99450243888797, 'epoch': 5.0})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒè€—æ—¶çº¦ 10 å°æ—¶ 36 åˆ†é’Ÿï¼Œè®­ç»ƒ 10 å°æ—¶çš„æˆæœ¬çº¦ä¸º `13.22 ç¾å…ƒ`ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚æœ[åœ¨ FLAN-T5-XXL ä¸Šè¿›è¡Œå…¨æ¨¡å‹å¾®è°ƒ](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) 10 ä¸ªå°æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ 8 ä¸ª A100 40GBï¼Œæˆæœ¬çº¦ä¸º 322 ç¾å…ƒã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹ä¿å­˜ä¸‹æ¥ä»¥ç”¨äºåé¢çš„æ¨ç†å’Œè¯„ä¼°ã€‚æˆ‘ä»¬æš‚æ—¶å°†å…¶ä¿å­˜åˆ°ç£ç›˜ï¼Œä½†ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ `model.push_to_hub` æ–¹æ³•å°†å…¶ä¸Šä¼ åˆ° [Hugging Face Hub](https://huggingface.co/docs/hub/main)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "('results\\\\tokenizer_config.json',\n 'results\\\\special_tokens_map.json',\n 'results\\\\spiece.model',\n 'results\\\\added_tokens.json',\n 'results\\\\tokenizer.json')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "peft_model_id=\"results\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "# if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åç”Ÿæˆçš„ LoRA checkpoint æ–‡ä»¶å¾ˆå°ï¼Œä»…éœ€ 84MB å°±åŒ…å«äº†ä» `samsum` æ•°æ®é›†ä¸Šå­¦åˆ°çš„æ‰€æœ‰çŸ¥è¯†ã€‚\n",
    "\n",
    "## 4. ä½¿ç”¨ LoRA FLAN-T5 è¿›è¡Œè¯„ä¼°å’Œæ¨ç†\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨ `evaluate` åº“æ¥è¯„ä¼° `rogue` åˆ†æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `PEFT` å’Œ `transformers` æ¥å¯¹ FLAN-T5 XXL æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å¯¹ FLAN-T5 XXL æ¨¡å‹ï¼Œæˆ‘ä»¬è‡³å°‘éœ€è¦ 18GB çš„â€‹â€‹ GPU æ˜¾å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1eecd27077804fb0afc36d4f7f0d843e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88f3b6cab9dd4f20ba32f710991fd088"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python3.7.9\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\ProgramData\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49682f9d9eb04527b0462ad87d850fbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffa0de3554594ad88865dacf370398ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c8e77e8cb8d4c059ded52cc9f39e80a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_id = \"results\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ç”¨æµ‹è¯•æ•°æ®é›†ä¸­çš„ä¸€ä¸ªéšæœºæ ·æœ¬æ¥è¯•è¯•æ‘˜è¦æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (D:/ProgramData/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32b636fa7e4a4df686454f15ad1bba99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: Ana: You sleeping?\r\n",
      "Catherine: Not yet.\r\n",
      "Ana: Wanna go visit grandma tomorrow? I miss her.\r\n",
      "Catherine: Yeah that would be nice :) I'll call you when I wake up\r\n",
      "Ana: Oki :) sleep well, good night.\r\n",
      "Catherine:  Good night, u too.\n",
      "------------------------------------------------------------\n",
      "summary:\n",
      "Catherine will call Ana in the morning. Ana and\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "dataset = load_dataset(\"samsum\")\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "\n",
    "input_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)\n",
    "print(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n",
    "\n",
    "print(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸é”™ï¼æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹ï¼Œå¹¶ä½¿ç”¨ `test` é›†ä¸­çš„å…¨éƒ¨æ•°æ®å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸€äº›å·¥å…·å‡½æ•°æ¥å¸®åŠ©ç”Ÿæˆæ‘˜è¦å¹¶å°†å…¶ä¸ç›¸åº”çš„å‚è€ƒæ‘˜è¦ç»„åˆåˆ°ä¸€èµ·ã€‚è¯„ä¼°æ‘˜è¦ä»»åŠ¡æœ€å¸¸ç”¨çš„æŒ‡æ ‡æ˜¯ [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric))ï¼Œå®ƒçš„å…¨ç§°æ˜¯ Recall-Oriented Understudy for Gisting Evaluationã€‚ä¸å¸¸ç”¨çš„å‡†ç¡®ç‡æŒ‡æ ‡ä¸åŒï¼Œå®ƒå°†ç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€ç»„å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 819/819 [1:16:58<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue1: 50.738340%\n",
      "rouge2: 24.992193%\n",
      "rougeL: 41.548080%\n",
      "rougeLsum: 41.518933%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_peft_model(sample,max_target_length=50):\n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "\n",
    "# load test dataset from distk\n",
    "test_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n",
    "\n",
    "# run predictions\n",
    "# this can take ~45 minutes\n",
    "predictions, references = [] , []\n",
    "for sample in tqdm(test_dataset):\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "\n",
    "# compute metric \n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# print results \n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
    "\n",
    "# Rogue1: 50.386161%\n",
    "# rouge2: 24.842412%\n",
    "# rougeL: 41.370130%\n",
    "# rougeLsum: 41.394230%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ PEFT å¾®è°ƒåçš„ â€‹â€‹FLAN-T5-XXL åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº† `50.38%` çš„ rogue1 åˆ†æ•°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ[flan-t5-base çš„å…¨æ¨¡å‹å¾®è°ƒè·å¾—äº† 47.23 çš„ rouge1 åˆ†æ•°](https://www.philschmid.de/fine-tune-flan-t5)ã€‚rouge1 åˆ†æ•°æé«˜äº† `3%` ã€‚\n",
    "\n",
    "ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ LoRA checkpoint åªæœ‰ 84MBï¼Œè€Œä¸”æ€§èƒ½æ¯”å¯¹æ›´å°çš„æ¨¡å‹è¿›è¡Œå…¨æ¨¡å‹å¾®è°ƒåçš„ checkpoint æ›´å¥½ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> è‹±æ–‡åŸæ–‡: <url> https://www.philschmid.de/fine-tune-flan-t5-peft </url>\n",
    "\n",
    "> åŸæ–‡ä½œè€…ï¼šPhilipp Schmid\n",
    "\n",
    "> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡å‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒæ¨ç†ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
