{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LoRA 和 Hugging Face 高效训练大语言模型\n",
    "\n",
    "在本文中，我们将展示如何使用[大语言模型低秩适配（Low-Rank Adaptation of Large Language Models，LoRA）](https://arxiv.org/abs/2106.09685)技术在单 GPU 上微调 110 亿参数的 FLAN-T5 XXL 模型。在此过程中，我们会使用到 Hugging Face 的 [Transformers](https://huggingface.co/docs/transformers/index)、[Accelerate](https://huggingface.co/docs/accelerate/index) 和 [PEFT](https://github.com/huggingface/peft) 库。\n",
    "\n",
    "通过本文，你会学到：\n",
    "\n",
    "1. 如何搭建开发环境\n",
    "2. 如何加载并准备数据集\n",
    "3. 如何使用 LoRA 和 bnb（即bitsandbytes） int-8 微调 T5\n",
    "4. 如何评估 LoRA FLAN-T5 并将其用于推理\n",
    "5. 如何比较不同方案的性价比\n",
    "\n",
    "### 快速入门：轻量化微调（Parameter Efficient Fine-Tuning，PEFT）\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft) 是 Hugging Face 的一个新的开源库。使用 PEFT 库，无需微调模型的全部参数，即可高效地将预训练语言模型 (Pre-trained Language Model，PLM) 适配到各种下游应用。PEFT 目前支持以下几种方法：\n",
    "\n",
    "- LoRA：[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning：[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning：[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning：[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "\n",
    "*注意：本教程是在 g5.2xlarge AWS EC2 实例上创建和运行的，该实例包含 1 个 NVIDIA A10G。*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 搭建开发环境\n",
    "\n",
    "在本例中，我们使用 AWS 预置的 [PyTorch 深度学习 AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html)，其已安装了正确的 CUDA 驱动程序和 PyTorch。在此基础上，我们还需要安装一些 Hugging Face 库，包括 transformers 和 datasets。运行下面的代码就可安装所有需要的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to c:\\users\\11985\\appdata\\local\\temp\\pip-req-build-3bwf80qe\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 34027fe813756897767b9a6f19ae7f1c4c7b418c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (23.0)\n",
      "Requirement already satisfied: psutil in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (1.13.1+cu116)\n",
      "Requirement already satisfied: transformers in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (4.26.1)\n",
      "Requirement already satisfied: accelerate in d:\\program files\\python3.7.9\\lib\\site-packages (from peft==0.3.0.dev0) (0.17.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\program files\\python3.7.9\\lib\\site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: filelock in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: requests in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\program files\\python3.7.9\\lib\\site-packages (from transformers->peft==0.3.0.dev0) (6.0.0)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python3.7.9\\lib\\site-packages (from tqdm>=4.27->transformers->peft==0.3.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\program files\\python3.7.9\\lib\\site-packages (from importlib-metadata->transformers->peft==0.3.0.dev0) (3.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests->transformers->peft==0.3.0.dev0) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests->transformers->peft==0.3.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests->transformers->peft==0.3.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests->transformers->peft==0.3.0.dev0) (2022.12.7)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml): started\n",
      "  Building wheel for peft (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=50810 sha256=b4a898041e90c0419af2e2eb4da857ded2f8390f40416a90cc7311624c9c766c\n",
      "  Stored in directory: C:\\Users\\11985\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-bgzptlev\\wheels\\13\\73\\6d\\ff27a3703d8bad21d7e0c24cbd9dde5d7ae78f756405707a0c\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.3.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git 'C:\\Users\\11985\\AppData\\Local\\Temp\\pip-req-build-3bwf80qe'\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in d:\\program files\\python3.7.9\\lib\\site-packages (0.1.2)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 142.7 kB/s eta 0:00:00\n",
      "Collecting py7zr\n",
      "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
      "     -------------------------------------- 66.4/66.4 kB 905.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py in d:\\program files\\python3.7.9\\lib\\site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in d:\\program files\\python3.7.9\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in d:\\program files\\python3.7.9\\lib\\site-packages (from rouge-score) (1.21.6)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Collecting grpcio>=1.24.3 (from tensorboard)\n",
      "  Downloading grpcio-1.54.0-cp37-cp37m-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 2.4 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard)\n",
      "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "     -------------------------------------- 178.2/178.2 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.9/93.9 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in d:\\program files\\python3.7.9\\lib\\site-packages (from tensorboard) (3.19.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from tensorboard) (2.28.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\program files\\python3.7.9\\lib\\site-packages (from tensorboard) (47.1.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "     -------------------------------------- 233.6/233.6 kB 7.2 MB/s eta 0:00:00\n",
      "Collecting wheel>=0.26 (from tensorboard)\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting texttable (from py7zr)\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pycryptodomex>=3.6.6 (from py7zr)\n",
      "  Downloading pycryptodomex-3.17-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting pyzstd>=0.14.4 (from py7zr)\n",
      "  Downloading pyzstd-0.15.6-cp37-cp37m-win_amd64.whl (231 kB)\n",
      "     -------------------------------------- 231.8/231.8 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n",
      "  Downloading pyppmd-1.0.0-cp37-cp37m-win_amd64.whl (46 kB)\n",
      "     ---------------------------------------- 46.1/46.1 kB ? eta 0:00:00\n",
      "Collecting pybcj>=0.6.0 (from py7zr)\n",
      "  Downloading pybcj-1.0.1-cp37-cp37m-win_amd64.whl (24 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.0.9 (from py7zr)\n",
      "  Downloading Brotli-1.0.9-cp37-cp37m-win_amd64.whl (365 kB)\n",
      "     -------------------------------------- 365.3/365.3 kB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata in d:\\program files\\python3.7.9\\lib\\site-packages (from py7zr) (6.0.0)\n",
      "Collecting inflate64>=0.3.1 (from py7zr)\n",
      "  Downloading inflate64-0.3.1-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: psutil in d:\\program files\\python3.7.9\\lib\\site-packages (from py7zr) (5.9.4)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\program files\\python3.7.9\\lib\\site-packages (from importlib-metadata->py7zr) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in d:\\program files\\python3.7.9\\lib\\site-packages (from importlib-metadata->py7zr) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program files\\python3.7.9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\program files\\python3.7.9\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: click in d:\\program files\\python3.7.9\\lib\\site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: joblib in d:\\program files\\python3.7.9\\lib\\site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\program files\\python3.7.9\\lib\\site-packages (from nltk->rouge-score) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in d:\\program files\\python3.7.9\\lib\\site-packages (from nltk->rouge-score) (4.65.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard)\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in d:\\program files\\python3.7.9\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Installing collected packages: texttable, tensorboard-plugin-wit, pyasn1, brotli, wheel, werkzeug, tensorboard-data-server, rsa, pyzstd, pyppmd, pycryptodomex, pyasn1-modules, oauthlib, multivolumefile, grpcio, cachetools, requests-oauthlib, pybcj, markdown, inflate64, google-auth, py7zr, google-auth-oauthlib, tensorboard\n",
      "Successfully installed brotli-1.0.9 cachetools-5.3.0 google-auth-2.17.3 google-auth-oauthlib-0.4.6 grpcio-1.54.0 inflate64-0.3.1 markdown-3.4.3 multivolumefile-0.2.3 oauthlib-3.2.2 py7zr-0.20.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 pybcj-1.0.1 pycryptodomex-3.17 pyppmd-1.0.0 pyzstd-0.15.6 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 texttable-1.6.7 werkzeug-2.2.3 wheel-0.40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\program files\\python3.7.9\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# install Hugging Face Libraries\n",
    "!pip install  git+https://github.com/huggingface/peft.git\n",
    "!pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "# install additional dependencies needed for training\n",
    "!pip install rouge-score tensorboard py7zr "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.加载并准备数据集\n",
    "\n",
    "这里，我们使用 [samsum](https://huggingface.co/datasets/samsum) 数据集，该数据集包含大约 16k 个含摘要的聊天类对话数据。这些对话由精通英语的语言学家制作。\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```\n",
    "\n",
    "我们使用 🤗 Datasets 库中的 *​`load_dataset()`* 方法来加载 `samsum` 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (D:/ProgramData/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f062668910240f4a157fdc9eb8e7608"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Train dataset size: 14732\n",
    "# Test dataset size: 819"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了训练模型，我们要用 🤗 Transformers Tokenizer 将输入文本转换为词元 ID。如果你需要了解这一方面的知识，请移步 Hugging Face 课程的 **[第 6 章](https://huggingface.co/course/chapter6/1?fw=tf)**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-xxl\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-XL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始训练之前，我们还需要对数据进行预处理。生成式文本摘要属于文本生成任务。我们将文本输入给模型，模型会输出摘要。我们需要了解输入和输出文本的长度信息，以利于我们高效地批量处理这些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\ProgramData\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\\cache-03b5cc31bf151e08.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 255\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a41ec54c3fb4bbbb3c3c913fc294285"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 50\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将在训练前统一对数据集进行预处理并将预处理后的数据集保存到磁盘。你可以在本地机器或 CPU 上运行此步骤并将其上传到 [Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\ProgramData\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\\cache-df829843b4810807.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26495fabc22646a4aaa89a1e2f3e1aca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:\\ProgramData\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e\\cache-c6d129a5cd66b800.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/14732 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7eaa85b31dd4c00a13cc8bd62b4703a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99d952f458f840ce92a56651eb2c6c38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 使用 LoRA 和 bnb int-8 微调 T5\n",
    "\n",
    "除了 LoRA 技术，我们还使用 [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) 把冻结的 LLM 量化为 int8。这使我们能够将 FLAN-T5 XXL 所需的内存降低到约四分之一。\n",
    "\n",
    "训练的第一步是加载模型。我们使用 [philschmid/flan-t5-xxl-sharded-fp16](https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16) 模型，它是 [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) 的分片版。分片可以让我们在加载模型时不耗尽内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "299e297a01cf4e888e7570a145caf076"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以使用 `peft` 为 LoRA int-8 训练作准备了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r=16, \n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如你所见，这里我们只训练了模型参数的 0.16%！这个巨大的内存增益让我们安心地微调模型，而不用担心内存问题。\n",
    "\n",
    "接下来需要创建一个 `DataCollat​​or`，负责对输入和标签进行填充，我们使用 🤗 Transformers 库中的`DataCollat​​orForSeq2Seq` 来完成这一环节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步是定义训练超参 (`TrainingArguments`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"lora-flan-t5-xxl\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\t\tauto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码，开始训练模型。请注意，对于 T5，出于收敛稳定性考量，某些层我们仍保持 `float32` 精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python3.7.9\\lib\\site-packages\\transformers\\optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='9210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/9210 : < :, Epoch 0.00/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=9210, training_loss=0.99450243888797, metrics={'train_runtime': 22422.9438, 'train_samples_per_second': 3.285, 'train_steps_per_second': 0.411, 'total_flos': 1.2471170806421914e+18, 'train_loss': 0.99450243888797, 'epoch': 5.0})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练耗时约 10 小时 36 分钟，训练 10 小时的成本约为 `13.22 美元`。相比之下，如果[在 FLAN-T5-XXL 上进行全模型微调](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) 10 个小时，我们需要 8 个 A100 40GB，成本约为 322 美元。\n",
    "\n",
    "我们可以将模型保存下来以用于后面的推理和评估。我们暂时将其保存到磁盘，但你也可以使用 `model.push_to_hub` 方法将其上传到 [Hugging Face Hub](https://huggingface.co/docs/hub/main)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "('results\\\\tokenizer_config.json',\n 'results\\\\special_tokens_map.json',\n 'results\\\\spiece.model',\n 'results\\\\added_tokens.json',\n 'results\\\\tokenizer.json')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "peft_model_id=\"results\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "# if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后生成的 LoRA checkpoint 文件很小，仅需 84MB 就包含了从 `samsum` 数据集上学到的所有知识。\n",
    "\n",
    "## 4. 使用 LoRA FLAN-T5 进行评估和推理\n",
    "\n",
    "我们将使用 `evaluate` 库来评估 `rogue` 分数。我们可以使用 `PEFT` 和 `transformers` 来对 FLAN-T5 XXL 模型进行推理。对 FLAN-T5 XXL 模型，我们至少需要 18GB 的​​ GPU 显存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary d:\\program files\\python3.7.9\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1eecd27077804fb0afc36d4f7f0d843e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88f3b6cab9dd4f20ba32f710991fd088"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python3.7.9\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\ProgramData\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49682f9d9eb04527b0462ad87d850fbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffa0de3554594ad88865dacf370398ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c8e77e8cb8d4c059ded52cc9f39e80a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_id = \"results\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用测试数据集中的一个随机样本来试试摘要效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (D:/ProgramData/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32b636fa7e4a4df686454f15ad1bba99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: Ana: You sleeping?\r\n",
      "Catherine: Not yet.\r\n",
      "Ana: Wanna go visit grandma tomorrow? I miss her.\r\n",
      "Catherine: Yeah that would be nice :) I'll call you when I wake up\r\n",
      "Ana: Oki :) sleep well, good night.\r\n",
      "Catherine:  Good night, u too.\n",
      "------------------------------------------------------------\n",
      "summary:\n",
      "Catherine will call Ana in the morning. Ana and\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "dataset = load_dataset(\"samsum\")\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "\n",
    "input_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)\n",
    "print(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n",
    "\n",
    "print(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不错！我们的模型有效！现在，让我们仔细看看，并使用 `test` 集中的全部数据对其进行评估。为此，我们需要实现一些工具函数来帮助生成摘要并将其与相应的参考摘要组合到一起。评估摘要任务最常用的指标是 [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric))，它的全称是 Recall-Oriented Understudy for Gisting Evaluation。与常用的准确率指标不同，它将生成的摘要与一组参考摘要进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819/819 [1:16:58<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue1: 50.738340%\n",
      "rouge2: 24.992193%\n",
      "rougeL: 41.548080%\n",
      "rougeLsum: 41.518933%\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_peft_model(sample,max_target_length=50):\n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "\n",
    "# load test dataset from distk\n",
    "test_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n",
    "\n",
    "# run predictions\n",
    "# this can take ~45 minutes\n",
    "predictions, references = [] , []\n",
    "for sample in tqdm(test_dataset):\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "\n",
    "# compute metric \n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# print results \n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
    "\n",
    "# Rogue1: 50.386161%\n",
    "# rouge2: 24.842412%\n",
    "# rougeL: 41.370130%\n",
    "# rougeLsum: 41.394230%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们 PEFT 微调后的 ​​FLAN-T5-XXL 在测试集上取得了 `50.38%` 的 rogue1 分数。相比之下，[flan-t5-base 的全模型微调获得了 47.23 的 rouge1 分数](https://www.philschmid.de/fine-tune-flan-t5)。rouge1 分数提高了 `3%` 。\n",
    "\n",
    "令人难以置信的是，我们的 LoRA checkpoint 只有 84MB，而且性能比对更小的模型进行全模型微调后的 checkpoint 更好。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 英文原文: <url> https://www.philschmid.de/fine-tune-flan-t5-peft </url>\n",
    "\n",
    "> 原文作者：Philipp Schmid\n",
    "\n",
    "> 译者: Matrix Yao (姚伟峰)，英特尔深度学习工程师，工作方向为 transformer-family 模型在各模态数据上的应用及大规模模型的训练推理。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
