{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.parse import DependencyGraph\n",
    "\n",
    "def get_subject_verb_object(sentence):\n",
    "    \"\"\"\n",
    "    Extracts subject, verb and object from sentence using NLTK dependency parser.\n",
    "    \"\"\"\n",
    "    # Load the parser\n",
    "    parser = nltk.parse.corenlp.CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "\n",
    "    # Parse the sentence\n",
    "    parse = next(parser.raw_parse(sentence)).to_conll(4).split('\\n')\n",
    "\n",
    "    # Extract subject, verb and object\n",
    "    subject = None\n",
    "    verb = None\n",
    "    obj = None\n",
    "\n",
    "    for p in parse:\n",
    "        if 'subj' in p:\n",
    "            subject = p.split('\\t')[2]\n",
    "        elif 'obj' in p:\n",
    "            obj = p.split('\\t')[2]\n",
    "        elif 'root' in p:\n",
    "            verb = p.split('\\t')[2]\n",
    "\n",
    "    return subject, verb, obj\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "subject, verb, obj = get_subject_verb_object(sentence)\n",
    "print(f\"Subject: {subject}, Verb: {verb}, Object: {obj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: fox, Verb: jumps, Object: dog\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def get_subject_verb_object(sentence):\n",
    "    \"\"\"\n",
    "    Extracts subject, verb and object from sentence using spaCy dependency parser.\n",
    "    \"\"\"\n",
    "    # Load the parser\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Parse the sentence\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract subject, verb and object\n",
    "    subject = None\n",
    "    verb = None\n",
    "    obj = None\n",
    "\n",
    "    for token in doc:\n",
    "        if 'subj' in token.dep_:\n",
    "            subject = token.text\n",
    "        elif 'obj' in token.dep_:\n",
    "            obj = token.text\n",
    "        elif 'ROOT' in token.dep_:\n",
    "            verb = token.text\n",
    "\n",
    "    return subject, verb, obj\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "subject, verb, obj = get_subject_verb_object(sentence)\n",
    "print(f\"Subject: {subject}, Verb: {verb}, Object: {obj}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_all_subject_verb_object(sentence):\n",
    "    \"\"\"\n",
    "    Extracts subject, verb and object from sentence using spaCy dependency parser.\n",
    "    \"\"\"\n",
    "    # Load the parser\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Parse the sentence\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract subject, verb and object\n",
    "    subject = []\n",
    "    verb = []\n",
    "    obj = []\n",
    "\n",
    "    for token in doc:\n",
    "        if 'subj' in token.dep_:\n",
    "            subject.append(token.text)\n",
    "        elif 'obj' in token.dep_:\n",
    "            obj.append(token.text)\n",
    "        elif 'ROOT' in token.dep_:\n",
    "            verb.append(token.text)\n",
    "\n",
    "    return subject, verb, obj"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_first_subject_verb_object(sentence):\n",
    "    \"\"\"\n",
    "    Extracts subject, verb and object from sentence using spaCy dependency parser.\n",
    "    \"\"\"\n",
    "    # Load the parser\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Parse the sentence\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract subject, verb and object\n",
    "    subject = \"\"\n",
    "    verb = \"\"\n",
    "    obj = \"\"\n",
    "\n",
    "    for token in doc:\n",
    "        if 'subj' in token.dep_ and not subject:\n",
    "            subject = token.text\n",
    "        elif 'obj' in token.dep_ and not obj:\n",
    "            obj = token.text\n",
    "        elif 'ROOT' in token.dep_ and not verb:\n",
    "            verb = token.text\n",
    "\n",
    "    return subject, verb, obj"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "sentence = \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "sentence = \"Young actor says he has no plans to fritter his cash away .\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "sentence = \"Radcliffe's earnings from first five Potter films have been held in trust fund .\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: ['Radcliffe', 'he'], Verb: ['gets'], Object: ['fortune']\n"
     ]
    }
   ],
   "source": [
    "subject, verb, obj = get_all_subject_verb_object(sentence)\n",
    "print(f\"Subject: {subject}, Verb: {verb}, Object: {obj}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_subject_verb_obj_new_label(sents):\n",
    "    subjects, verbs, objs = [], [], []\n",
    "    for sent in sent_tokenize(sents):\n",
    "        subject, verb, obj = get_first_subject_verb_object(sent)\n",
    "        subjects.append(subject)\n",
    "        verbs.append(verb)\n",
    "        objs.append(obj)\n",
    "    res = f\"Subject: {' '.join(subjects)} Verb: {' '.join(verbs)} Object: {' '.join(objs)} [SEP] {sents}\"\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def map_subject_verb_obj(example):\n",
    "    import spacy\n",
    "    summary_column = \"highlights\"\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    def get_first_subject_verb_object(sentence):\n",
    "        \"\"\"\n",
    "        Extracts subject, verb and object from sentence using spaCy dependency parser.\n",
    "        \"\"\"\n",
    "        # Load the parser\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "        # Parse the sentence\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        # Extract subject, verb and object\n",
    "        subject = \"\"\n",
    "        verb = \"\"\n",
    "        obj = \"\"\n",
    "\n",
    "        for token in doc:\n",
    "            if 'subj' in token.dep_ and not subject:\n",
    "                subject = token.text\n",
    "            elif 'obj' in token.dep_ and not obj:\n",
    "                obj = token.text\n",
    "            elif 'ROOT' in token.dep_ and not verb:\n",
    "                verb = token.text\n",
    "\n",
    "        return subject, verb, obj\n",
    "    def get_subject_verb_obj_new_label(sents):\n",
    "        subjects, verbs, objs = [], [], []\n",
    "        for sent in sent_tokenize(sents):\n",
    "            subject, verb, obj = get_first_subject_verb_object(sent)\n",
    "            subjects.append(subject)\n",
    "            verbs.append(verb)\n",
    "            objs.append(obj)\n",
    "        res = f\"Subject: {' '.join(subjects)} Verb: {' '.join(verbs)} Object: {' '.join(objs)} [SEP] {sents}\"\n",
    "        return res\n",
    "    return {\"summary_column\": get_subject_verb_obj_new_label(example[summary_column])}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def batch_map_all_subject_verb_obj(examples):\n",
    "    import spacy\n",
    "    summary_column = \"highlights\"\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    # Load the parser\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    def get_all_subject_verb_object(sentence):\n",
    "        \"\"\"\n",
    "        Extracts subject, verb and object from sentence using spaCy dependency parser.\n",
    "        \"\"\"\n",
    "        # Load the parser\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "        # Parse the sentence\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        # Extract subject, verb and object\n",
    "        subject = []\n",
    "        verb = []\n",
    "        obj = []\n",
    "\n",
    "        for token in doc:\n",
    "            if 'subj' in token.dep_:\n",
    "                subject.append(token.text)\n",
    "            elif 'obj' in token.dep_:\n",
    "                obj.append(token.text)\n",
    "            elif 'ROOT' in token.dep_:\n",
    "                verb.append(token.text)\n",
    "\n",
    "        return subject, verb, obj\n",
    "    def get_subject_verb_obj_new_label(sents):\n",
    "        subjects, verbs, objs = [], [], []\n",
    "        for sent in sent_tokenize(sents):\n",
    "            subject, verb, obj = get_all_subject_verb_object(sent)\n",
    "            subjects.extend(subject)\n",
    "            verbs.extend(verb)\n",
    "            objs.extend(obj)\n",
    "        res = f\"Subject: {' '.join(subjects)} Verb: {' '.join(verbs)} Object: {' '.join(objs)} [SEP] {sents}\"\n",
    "        return res\n",
    "    summarys = []\n",
    "    for summary in examples[summary_column]:\n",
    "        summarys.append(get_subject_verb_obj_new_label(summary))\n",
    "    return {\"summary_column\": summarys}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound\n",
      "compound\n",
      "compound\n",
      "compound\n",
      "nsubj\n",
      "ROOT\n",
      "nmod\n",
      "nummod\n",
      "compound\n",
      "dobj\n",
      "mark\n",
      "nsubj\n",
      "advcl\n",
      "nummod\n",
      "npadvmod\n",
      "punct\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Parse the sentence\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(token.dep_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday ."
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (D:/ProgramData/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e9c5c2ebd72485787b6a99213f137e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .',\n 'Young actor says he has no plans to fritter his cash away .',\n \"Radcliffe's earnings from first five Potter films have been held in trust fund .\"]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"highlights\"].split(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Subject: Radcliffe actor earnings Verb: gets says held Object: fortune plans films [SEP] Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\""
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subject_verb_obj_new_label(dataset[\"train\"][0][\"highlights\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "['Mentally ill inmates in Miami are housed on the \"forgotten floor\".',\n 'Judge Steven Leifman says most are there as a result of \"avoidable felonies\".',\n 'While CNN tours facility, patient shouts: \"I am the son of the president\".',\n \"Leifman says the system is unjust and he's fighting for change .\"]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(dataset[\"train\"][1][\"highlights\"].replace(\"\\n\", \". \\n\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               "
     ]
    }
   ],
   "source": [
    "sub_dataset = dataset.map(batch_map_all_subject_verb_obj, batched=True, num_proc=15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: none President Bush Verb: says reclaims undergoes Object: procedure powers colonoscopy [SEP] Five small polyps found during procedure; \"none worrisome,\" spokesman says .\n",
      "President reclaims powers transferred to vice president .\n",
      "Bush undergoes routine colonoscopy at Camp David .\n"
     ]
    }
   ],
   "source": [
    "print(sub_dataset[\"train\"][3][\"summary_column\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five nummod\n",
      "[]\n",
      "small amod\n",
      "[]\n",
      "polyps ccomp\n",
      "[Five, small, found, ;, worrisome]\n",
      "found acl\n",
      "[during]\n",
      "during prep\n",
      "[procedure]\n",
      "procedure pobj\n",
      "[]\n",
      "; punct\n",
      "[]\n",
      "\" punct\n",
      "[]\n",
      "none nsubj\n",
      "[]\n",
      "worrisome amod\n",
      "[\", none]\n",
      ", punct\n",
      "[]\n",
      "\" punct\n",
      "[]\n",
      "spokesman nsubj\n",
      "[]\n",
      "says ROOT\n",
      "[polyps, ,, \", spokesman, .]\n",
      ". punct\n",
      "[\n",
      "]\n",
      "\n",
      " dep\n",
      "[]\n",
      "President nsubj\n",
      "[]\n",
      "reclaims ROOT\n",
      "[President, powers, .]\n",
      "powers dobj\n",
      "[transferred]\n",
      "transferred acl\n",
      "[to]\n",
      "to prep\n",
      "[president]\n",
      "vice compound\n",
      "[]\n",
      "president pobj\n",
      "[vice]\n",
      ". punct\n",
      "[\n",
      "]\n",
      "\n",
      " dep\n",
      "[]\n",
      "Bush nsubj\n",
      "[]\n",
      "undergoes ROOT\n",
      "[Bush, colonoscopy, at, .]\n",
      "routine amod\n",
      "[]\n",
      "colonoscopy dobj\n",
      "[routine]\n",
      "at prep\n",
      "[David]\n",
      "Camp compound\n",
      "[]\n",
      "David pobj\n",
      "[Camp]\n",
      ". punct\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = sub_dataset[\"train\"][3][\"highlights\"]\n",
    "# Parse the sentence\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(token, token.dep_)\n",
    "    print([i for i in token.children])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.batch_decode([])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 47159, 35, 4146, 270, 3516, 38132, 35, 161, 20507, 29, 10946, 293, 35671, 35, 7089, 4361, 17735, 17591, 16572, 1437, 2, 4934, 650, 11424, 3275, 303, 148, 7089, 131, 22, 39763, 29611, 60, 1565, 161, 479, 50118, 6517, 20507, 29, 4361, 7225, 7, 2626, 394, 479, 50118, 43294, 10946, 293, 6108, 17735, 17591, 16572, 23, 4746, 871, 479, 2]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(sub_dataset[\"train\"][3][\"summary_column\"].replace(\"[SEP]\", \"</s>\"))[\"input_ids\"]\n",
    "print(input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_30640\\596540006.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32md:\\program files\\python3.7.9\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36mdecode\u001B[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m   3478\u001B[0m             \u001B[0mskip_special_tokens\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mskip_special_tokens\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3479\u001B[0m             \u001B[0mclean_up_tokenization_spaces\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mclean_up_tokenization_spaces\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3480\u001B[1;33m             \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3481\u001B[0m         )\n\u001B[0;32m   3482\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\program files\\python3.7.9\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001B[0m in \u001B[0;36m_decode\u001B[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m    547\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    548\u001B[0m             \u001B[0mtoken_ids\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtoken_ids\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 549\u001B[1;33m         \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mskip_special_tokens\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mskip_special_tokens\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    550\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    551\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mclean_up_tokenization_spaces\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "['<s>',\n 'Subject',\n ':',\n 'Ġnone',\n 'ĠPresident',\n 'ĠBush',\n 'ĠVerb',\n ':',\n 'Ġsays',\n 'Ġreclaim',\n 's',\n 'Ġundergo',\n 'es',\n 'ĠObject',\n ':',\n 'Ġprocedure',\n 'Ġpowers',\n 'Ġcolon',\n 'osc',\n 'opy',\n 'Ġ',\n '</s>',\n 'ĠFive',\n 'Ġsmall',\n 'Ġpoly',\n 'ps',\n 'Ġfound',\n 'Ġduring',\n 'Ġprocedure',\n ';',\n 'Ġ\"',\n 'none',\n 'Ġworrisome',\n ',\"',\n 'Ġspokesman',\n 'Ġsays',\n 'Ġ.',\n 'Ċ',\n 'President',\n 'Ġreclaim',\n 's',\n 'Ġpowers',\n 'Ġtransferred',\n 'Ġto',\n 'Ġvice',\n 'Ġpresident',\n 'Ġ.',\n 'Ċ',\n 'Bush',\n 'Ġundergo',\n 'es',\n 'Ġroutine',\n 'Ġcolon',\n 'osc',\n 'opy',\n 'Ġat',\n 'ĠCamp',\n 'ĠDavid',\n 'Ġ.',\n '</s>']"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bos_token': '<s>',\n 'eos_token': '</s>',\n 'unk_token': '<unk>',\n 'sep_token': '</s>',\n 'pad_token': '<pad>',\n 'cls_token': '<s>',\n 'mask_token': '<mask>',\n 'additional_special_tokens': ['<sep>']}"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<sep>\"]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "50265"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"<sep>\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.sp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "'</s>'"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map['sep_token']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [0, 48600, 35, 2], 'attention_mask': [1, 1, 1, 1]}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"summary:\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [0, 50118, 2], 'attention_mask': [1, 1, 1]}"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[   0,  102, 1437,    2,  741,  740,    2],\n        [   0,  102,  741, 1437,    2,  740,    2]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer([\"a </s> b c\", \"a b </s> c\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "input_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[    0,   102,  1437, 50265,   741,   740,     2],\n        [    0,   102,   741,  1437, 50265,   740,     2]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer([\"a <sep> b c\", \"a b <sep> c\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "input_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[    0,   102,  1437, 50265,     2],\n        [    0, 50265,    10,     2,     1]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer([\"a <sep>\", \"<sep> a\"], return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "input_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "'Ġ'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1437)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "['<s>a <sep></s>', '<s><sep> a</s><pad>']"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False, False, False,  True, False, False,  True],\n        [False, False, False, False,  True, False,  True]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids == 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  2],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "mask = (x == 2).cumsum(dim=1) == 1\n",
    "x.masked_fill_(mask, 2)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[    0,     0,     0, 50265,     2],\n        [    0, 50265,    10,     2,     1]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "mask = (input_ids == 50265).cumsum(dim=1) == 0\n",
    "input_ids.masked_fill_(mask, 0)\n",
    "input_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Subject: Klans</s>nic he supervisors board Verb: says treated refuses Object: issue years principal him orientation comment decision contract <sep> Tom Klansnic says he has never heard any issue or complaint during his 10 years as principal.\n",
      "Claims his supervisors have treated him differently since coming out on his sexual orientation.\n",
      "School board refuses comment or explanation for decision not to renew his contract.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([0, 47159, 35, 7507, 1253, 2, 12979, 37, 19971, 792, 38132, 35, 161, 3032, 16766, 35671, 35, 696, 107, 5402, 123, 14497, 1129, 568, 1355, 1437, 50265, 1560, 7507, 1253, 12979, 161, 37, 34, 393, 1317, 143, 696, 50, 3674, 148, 39, 158, 107\n",
    ", 25, 5402, 479, 50118, 45699, 29, 39, 19971, 33, 3032, 123, 8225, 187, 567, 66, 15, 39, 1363, 14497, 479, 50118, 26751, 792, 16766, 1129, 50, 8257, 13, 568, 45, 7, 11007, 39, 1355, 479, 2]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  8],\n",
      "        [ 0,  0,  0,  0],\n",
      "        [ 0,  0, 15, 16]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def set_second_zero(tensor):\n",
    "    for i in range(tensor.shape[0]):\n",
    "        zero_count = 0\n",
    "        for j in range(tensor.shape[1]):\n",
    "            if tensor[i][j] == 0:\n",
    "                zero_count += 1\n",
    "                if zero_count == 2:\n",
    "                    break\n",
    "            else:\n",
    "                tensor[i][j] = 0\n",
    "\n",
    "    return tensor\n",
    "\n",
    "tensor = torch.tensor([[0, 2, 3, 4], [0, 6, 0, 8], [0, 10, 11, 0], [0, 0, 15, 16]])\n",
    "print(set_second_zero(tensor))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  2,  3,  4],\n        [ 0,  6,  0,  8],\n        [ 0, 10, 11,  0],\n        [ 0,  0, 15, 16]])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([[0, 2, 3, 4], [0, 6, 0, 8], [0, 10, 11, 0], [0, 0, 15, 16]])\n",
    "tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ True, False, False, False],\n        [ True, False,  True, False],\n        [ True, False, False,  True],\n        [ True,  True, False, False]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor == 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "t = \"a b c summary: d e f\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "t = \"a b c\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "'a b c'"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = t.find(\"summary:\")\n",
    "t[pos if pos != -1 else 0:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos == False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos if pos != -1 else 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def batch_map_all_subject_verb_obj(examples):\n",
    "        import random\n",
    "        prompt_sep_token = \"Summary:\"\n",
    "        summary_column = \"highlights\"\n",
    "        import spacy\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        # Load the parser\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "        def get_all_subject_verb_object(sentence):\n",
    "            \"\"\"\n",
    "            Extracts subject, verb and object from sentence using spaCy dependency parser.\n",
    "            \"\"\"\n",
    "            # Parse the sentence\n",
    "            doc = nlp(sentence)\n",
    "\n",
    "            # Extract subject, verb and object\n",
    "            subject = []\n",
    "            verb = []\n",
    "            obj = []\n",
    "\n",
    "            for token in doc:\n",
    "                if 'subj' in token.dep_:\n",
    "                    subject.append(token.text)\n",
    "                elif 'obj' in token.dep_:\n",
    "                    obj.append(token.text)\n",
    "                elif 'ROOT' in token.dep_:\n",
    "                    verb.append(token.text)\n",
    "\n",
    "            return subject, verb, obj\n",
    "\n",
    "        def get_subject_verb_obj_new_label(sents):\n",
    "            subjects, verbs, objs = [], [], []\n",
    "            for sent in sent_tokenize(sents):\n",
    "                subject, verb, obj = get_all_subject_verb_object(sent)\n",
    "                subjects.extend(subject)\n",
    "                verbs.extend(verb)\n",
    "                objs.extend(obj)\n",
    "            # subjects = random.sample(subjects, min(len(subjects), 6))\n",
    "            # verbs = random.sample(verbs, min(len(verbs), 6))\n",
    "            # objs = random.sample(objs, min(len(objs), 6))\n",
    "            res = f\"Subjects: {', '.join(subjects)}. Predicate: {', '.join(verbs)}. Object: {', '.join(objs)}. {prompt_sep_token} {sents}\"\n",
    "            return res\n",
    "\n",
    "        summarys = []\n",
    "        for summary in examples[summary_column]:\n",
    "            summarys.append(get_subject_verb_obj_new_label(summary))\n",
    "        return {summary_column: summarys}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    }
   ],
   "source": [
    "sub_dataset = dataset.map(batch_map_all_subject_verb_obj, batched=True, num_proc=15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "out = open(\"ChatGLM-6B\\ptuning\\data\\cnn_dailymail_svo_train.json\", \"w\", encoding=\"utf-8\")\n",
    "for item in sub_dataset[\"train\"]:\n",
    "    out.write(json.dumps({\"article\": item[\"article\"], \"highlights\": item[\"highlights\"].replace(\"\\n\", \" \")}) + \"\\n\")\n",
    "out.close()\n",
    "\n",
    "out = open(\"ChatGLM-6B\\ptuning\\data\\cnn_dailymail_svo_test.json\", \"w\", encoding=\"utf-8\")\n",
    "for item in sub_dataset[\"test\"]:\n",
    "    out.write(json.dumps({\"article\": item[\"article\"], \"highlights\": item[\"highlights\"].replace(\"\\n\", \" \")}) + \"\\n\")\n",
    "out.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "[3, 2]"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample([1, 2, 3, 4, 5], 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "highlights = \"Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June . Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "import random\n",
    "prompt_sep_token = \"Summary:\"\n",
    "summary_column = \"highlights\"\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# Load the parser\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_all_subject_verb_object(sentence):\n",
    "    \"\"\"\n",
    "    Extracts subject, verb and object from sentence using spaCy dependency parser.\n",
    "    \"\"\"\n",
    "    # Parse the sentence\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract subject, verb and object\n",
    "    subject = []\n",
    "    verb = []\n",
    "    obj = []\n",
    "\n",
    "    for token in doc:\n",
    "        print(token, token.dep_)\n",
    "        if 'subj' in token.dep_:\n",
    "            subject.append(token.text)\n",
    "        elif 'obj' in token.dep_:\n",
    "            obj.append(token.text)\n",
    "        elif 'ROOT' in token.dep_:\n",
    "            verb.append(token.text)\n",
    "\n",
    "    return subject, verb, obj\n",
    "\n",
    "def get_subject_verb_obj_new_label(sents):\n",
    "    subjects, verbs, objs = [], [], []\n",
    "    for sent in sent_tokenize(sents):\n",
    "        subject, verb, obj = get_all_subject_verb_object(sent)\n",
    "        subjects.extend(subject)\n",
    "        verbs.extend(verb)\n",
    "        objs.extend(obj)\n",
    "    # subjects = random.sample(subjects, min(len(subjects), 6))\n",
    "    # verbs = random.sample(verbs, min(len(verbs), 6))\n",
    "    # objs = random.sample(objs, min(len(objs), 6))\n",
    "    res = f\"Subjects: {','.join(subjects)}. Predicate: {','.join(verbs)}. Object: {','.join(objs)}. {prompt_sep_token} {sents}\"\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membership nsubj\n",
      "gives ROOT\n",
      "the det\n",
      "ICC compound\n",
      "jurisdiction dobj\n",
      "over prep\n",
      "alleged amod\n",
      "crimes pobj\n",
      "committed acl\n",
      "in prep\n",
      "Palestinian amod\n",
      "territories pobj\n",
      "since prep\n",
      "last amod\n",
      "June pobj\n",
      ". punct\n",
      "Israel nsubj\n",
      "and cc\n",
      "the det\n",
      "United compound\n",
      "States conj\n",
      "opposed ROOT\n",
      "the det\n",
      "move dobj\n",
      ", punct\n",
      "which nsubj\n",
      "could aux\n",
      "open relcl\n",
      "the det\n",
      "door dobj\n",
      "to prep\n",
      "war compound\n",
      "crimes compound\n",
      "investigations pobj\n",
      "against prep\n",
      "Israelis pobj\n",
      ". punct\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Subjects: Membership,Israel,which. Predicate: gives,opposed. Object: jurisdiction,crimes,territories,June,move,door,investigations,Israelis. Summary: Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June . Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subject_verb_obj_new_label(highlights)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
